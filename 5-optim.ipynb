{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # self.conv1 = Conv2d(3, 32, 5, padding=2)\n",
    "        # self.maxpool1 = MaxPool2d(2)\n",
    "        # self.conv2 = Conv2d(32, 32, 5, padding=2)\n",
    "        # self.maxpool2 = MaxPool2d(2)\n",
    "        # self.conv3 = Conv2d(32, 64, 5, padding=2)\n",
    "        # self.maxpool3 = MaxPool2d(2)\n",
    "        # self.flatten = Flatten()\n",
    "        # self.linear1 = Linear(1024, 64)\n",
    "        # self.linear2 = Linear(64, 10)\n",
    "\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.maxpool1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.maxpool2(x)\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.maxpool3(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.linear2(x)\n",
    "\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "print(model)\n",
    "input = torch.ones((64, 3, 32, 32))\n",
    "output = model(input)\n",
    "\n",
    "# 检查网络结构正确性\n",
    "print(output.shape)\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "writer.add_graph(model, input)\n",
    "writer.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6667)\n",
      "tensor(1.3333)\n",
      "tensor(1.1019)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minervaning/opt/anaconda3/envs/NLPbasic/lib/python3.9/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 1, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/Users/minervaning/opt/anaconda3/envs/NLPbasic/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 1, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import L1Loss, CrossEntropyLoss\n",
    "\n",
    "inputs = torch.tensor([1,2,3], dtype=torch.float32)\n",
    "targets = torch.tensor([1,2,5], dtype=torch.float32)\n",
    "\n",
    "inputs = torch.reshape(inputs, (1, 1, 1, 3))\n",
    "tragets = torch.reshape(targets, (1, 1, 1, 3))\n",
    "\n",
    "# L1距离\n",
    "loss = L1Loss()\n",
    "result = loss(inputs, targets)\n",
    "# L2距离\n",
    "loss_mse = nn.MSELoss()\n",
    "result_mse = loss_mse(inputs, targets)\n",
    "# 交叉熵: 限制各个class上的概率整体比较小\n",
    "x = torch.tensor([0.1, 0.2, 0.3])\n",
    "y = torch.tensor([1])\n",
    "x = torch.reshape(x, (1, 3)) # 一个维度，有三类\n",
    "\n",
    "loss_crossEntropy = CrossEntropyLoss()\n",
    "result_crossEntropy = loss_crossEntropy(x, y)\n",
    "\n",
    "print(result)\n",
    "print(result_mse)\n",
    "print(result_crossEntropy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function in network and optimizer\n",
    "\n",
    "lr: learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2137.9885, grad_fn=<AddBackward0>)\n",
      "tensor(1809.4391, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/minervaning/Documents/Projects/pytorch_learn/network.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minervaning/Documents/Projects/pytorch_learn/network.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m result_loss \u001b[39m=\u001b[39m loss(output, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minervaning/Documents/Projects/pytorch_learn/network.ipynb#W4sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()         \u001b[39m# 梯度清零，因为上一次的梯度对这一次没有用\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/minervaning/Documents/Projects/pytorch_learn/network.ipynb#W4sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m result_loss\u001b[39m.\u001b[39;49mbackward()  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minervaning/Documents/Projects/pytorch_learn/network.ipynb#W4sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()              \u001b[39m# 更新一次\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/minervaning/Documents/Projects/pytorch_learn/network.ipynb#W4sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# print(result_loss)  \u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLPbasic/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLPbasic/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import L1Loss, CrossEntropyLoss\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # self.conv1 = Conv2d(3, 32, 5, padding=2)\n",
    "        # self.maxpool1 = MaxPool2d(2)\n",
    "        # self.conv2 = Conv2d(32, 32, 5, padding=2)\n",
    "        # self.maxpool2 = MaxPool2d(2)\n",
    "        # self.conv3 = Conv2d(32, 64, 5, padding=2)\n",
    "        # self.maxpool3 = MaxPool2d(2)\n",
    "        # self.flatten = Flatten()\n",
    "        # self.linear1 = Linear(1024, 64)\n",
    "        # self.linear2 = Linear(64, 10)\n",
    "\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.maxpool1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.maxpool2(x)\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.maxpool3(x)\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.linear2(x)\n",
    "\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\"dataset\", train=False, transform=torchvision.transforms.ToTensor(), download=False)\n",
    "dataloader = DataLoader(dataset, batch_size=10)\n",
    "loss = CrossEntropyLoss()\n",
    "model = Model()\n",
    "# 定义优化器\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    # 相当于只学习了一轮\n",
    "    for data in dataloader:\n",
    "        imgs, targets = data\n",
    "        output = model(imgs)\n",
    "        result_loss = loss(output, targets)\n",
    "        optim.zero_grad()         # 梯度清零，因为上一次的梯度对这一次没有用\n",
    "        result_loss.backward()  \n",
    "        optim.step()              # 更新一次\n",
    "        # print(result_loss)  \n",
    "        running_loss = running_loss + result_loss\n",
    "\n",
    "    print(running_loss)    \n",
    "    \n",
    "# writer = SummaryWriter(\"logs\")\n",
    "# writer.add_graph(model, input)\n",
    "# writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPbasic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (default, Nov 15 2020, 06:25:35) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "882c1afb4c9a40325de8f81978e52d317fad24a18050be2528b8982770e49e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
